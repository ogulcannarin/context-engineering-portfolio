import os
from dotenv import load_dotenv

# GÃ¼venlik KontrolÃ¼
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    raise ValueError("âš ï¸ API Key bulunamadÄ±! .env dosyasÄ±nÄ± kontrol et.")

# ---- GÃœNCEL IMPORTLAR ----
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- 1. VERÄ° SETÄ° ---
raw_text = """
Buzul Kalesi GÃ¼venlik Raporu:
Tesisin ana gÃ¼Ã§ kaynaÄŸÄ± 'Zero-Point Energy' (ZPE) modÃ¼lÃ¼dÃ¼r.
ZPE modÃ¼lÃ¼ aÅŸÄ±rÄ± Ä±sÄ±nÄ±rsa, sistem otomatik olarak 'KÄ±zÄ±l KÄ±ÅŸ' (Red Winter) protokolÃ¼nÃ¼ baÅŸlatÄ±r.
KÄ±zÄ±l KÄ±ÅŸ protokolÃ¼, tÃ¼m kapÄ±larÄ± kilitler ve oksijen seviyesini %15'e dÃ¼ÅŸÃ¼rÃ¼r.
Bu protokolÃ¼ sadece 'YÃ¶netici Omega' yetki koduyla iptal edebilirsiniz.
Ä°ptal ÅŸifresi her gÃ¼n deÄŸiÅŸir, bugÃ¼nÃ¼n ÅŸifresi: 'Aurora-77'.
"""

print("âš™ï¸ VeritabanÄ± hazÄ±rlanÄ±yor...")

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=150,
    chunk_overlap=30
)

docs = text_splitter.create_documents([raw_text])

embeddings = OpenAIEmbeddings()
vector_db = Chroma.from_documents(docs, embeddings)

# LLM
llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0
)

# --- 2. MULTI QUERY ---
print("\n--- TEKNÄ°K 1: MULTI-QUERY ---")

multi_query_prompt = PromptTemplate.from_template("""
Sen bir yapay zeka asistanÄ±sÄ±n.

GÃ¶revin kullanÄ±cÄ± sorusunu vektÃ¶r aramasÄ± iÃ§in 3 farklÄ± ÅŸekilde yeniden yazmak.

Sadece 3 soru Ã¼ret.
AÃ§Ä±klama yapma.

Soru: {question}
""")

generate_queries_chain = (
    multi_query_prompt
    | llm
    | StrOutputParser()
    | (lambda x: x.split("\n"))
)

user_question = "Sistem Ä±sÄ±nÄ±rsa ne olur?"
print(f"ğŸ‘¤ KullanÄ±cÄ± Sorusu: {user_question}")

generated_queries = generate_queries_chain.invoke(
    {"question": user_question}
)

print("\nğŸ¤– Alternatif Sorular:")
for i, q in enumerate(generated_queries):
    print(f"{i+1}. {q}")

retrieved_docs = vector_db.similarity_search(
    generated_queries[0],
    k=1
)

print("\nğŸ” Bulunan Metin:")
print(retrieved_docs[0].page_content)

# --- 3. HYDE ---
print("\n--- TEKNÄ°K 2: HyDE ---")

hyde_prompt = PromptTemplate.from_template("""
Soruyu cevaplayan kÄ±sa hayali bir paragraf yaz.

Soru: {question}
""")

hyde_chain = hyde_prompt | llm | StrOutputParser()

bad_question = "Ä°ptal ÅŸifresi ne?"
print(f"\nğŸ‘¤ KullanÄ±cÄ± Sorusu: {bad_question}")

hypothetical_answer = hyde_chain.invoke(
    {"question": bad_question}
)

print("\nğŸ‘» Hayali Cevap:")
print(hypothetical_answer)

hyde_docs = vector_db.similarity_search(
    hypothetical_answer,
    k=1
)

print("\nâœ… GerÃ§ek Metin:")
print(hyde_docs[0].page_content)
