import os
from dotenv import load_dotenv

# --- GÃœVENLÄ°K ---
# .env dosyasÄ±nÄ± yÃ¼klÃ¼yoruz
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("âš ï¸ API Key bulunamadÄ±! LÃ¼tfen .env dosyasÄ±nÄ± kontrol et.")
os.environ["OPENAI_API_KEY"] = api_key

# --- KÃœTÃœPHANELER ---
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from rank_bm25 import BM25Okapi
from sentence_transformers import CrossEncoder

# --- 1. VERÄ° HAZIRLIÄI (Zorlu Senaryo) ---
# VektÃ¶rÃ¼n tek baÅŸÄ±na zorlanacaÄŸÄ±, iÃ§inde Ã¶zel kodlar olan bir metin.
raw_text = """
Gizli Proje: 'Project Chimera'. Kod adÄ±: X-99.
Bu proje, insan zihnini dijital ortama aktarmayÄ± hedefler.
X-99 sunucularÄ± sadece Antarktika'daki 'Buzul Kalesi'nde bulunur.
Sisteme giriÅŸ ÅŸifresi 'Mavi_Ufuk_2042'dir.
EÄŸer sistem aÅŸÄ±rÄ± Ä±sÄ±nÄ±rsa, acil durum protokolÃ¼ 'Protokol Omega' devreye girer.
Protokol Omega, tÃ¼m verileri siler ve tesisi kilitler.
Yetkili personel dÄ±ÅŸÄ±nda kimse 'BÃ¶lge 51-B'ye giremez.
"""

print(f"ğŸ“„ Metin uzunluÄŸu: {len(raw_text)} karakter")

# Chunking (ParÃ§alama)
text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)
docs = text_splitter.create_documents([raw_text])
print(f"ğŸ§© Toplam ParÃ§a SayÄ±sÄ±: {len(docs)}")

# --- 2. HÄ°BRÄ°T ARAMA MOTORLARINI KURMA ---

# A) VektÃ¶r VeritabanÄ± (Anlamsal Arama)
print("âš™ï¸  VektÃ¶r veritabanÄ± kuruluyor...")
embeddings = OpenAIEmbeddings()
vector_db = Chroma.from_documents(docs, embeddings)

# B) BM25 (Kelime BazlÄ± Arama)
# BM25 metni kelime kelime ayÄ±rmamÄ±zÄ± ister (Tokenization)
print("âš™ï¸  BM25 motoru kuruluyor...")
tokenized_corpus = [doc.page_content.lower().split(" ") for doc in docs]
bm25 = BM25Okapi(tokenized_corpus)

# C) Reranker (Yeniden SÄ±ralayÄ±cÄ±) - Cross Encoder
# Bu model soruyu ve cevabÄ± YANYANA okuyup puan verir.
print("â³ Reranker modeli indiriliyor (Bu ilk seferde biraz sÃ¼rebilir)...")
reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2') 
print("âœ… TÃ¼m sistemler hazÄ±r!\n")

# --- 3. ANA FONKSÄ°YON: HYBRID SEARCH + RERANKING ---
def search_system(query):
    print(f"ğŸ” SORU: '{query}'")
    
    # ADIM 1: GeniÅŸ Arama (Recall) - Hem VektÃ¶r hem Kelime ile Ã§ok sayÄ±da sonuÃ§ getir
    # VektÃ¶rden 3 tane al
    vector_results = vector_db.similarity_search(query, k=3)
    
    # BM25'ten 3 tane al
    tokenized_query = query.lower().split(" ")
    bm25_results = bm25.get_top_n(tokenized_query, docs, n=3)
    
    # Listeleri birleÅŸtir ve kopyalarÄ± temizle (Deduplication)
    combined_docs = []
    seen_contents = set()
    
    for doc in vector_results + bm25_results:
        if doc.page_content not in seen_contents:
            combined_docs.append(doc)
            seen_contents.add(doc.page_content)
            
    print(f"   ğŸ”¹ Ä°lk Havuz (VektÃ¶r + BM25): {len(combined_docs)} dÃ¶kÃ¼man bulundu.")

    # ADIM 2: Reranking (Precision) - Cross Encoder ile Puanlama
    # Modele ÅŸÃ¶yle soruyoruz: [Soru, Metin] ikilisi ne kadar alakalÄ±?
    pairs = [[query, doc.page_content] for doc in combined_docs]
    scores = reranker.predict(pairs)
    
    # SkorlarÄ± dÃ¶kÃ¼manlarla eÅŸleÅŸtirip sÄ±ralayalÄ±m (En yÃ¼ksek puan en Ã¼ste)
    ranked_results = sorted(zip(combined_docs, scores), key=lambda x: x[1], reverse=True)
    
    print("   ğŸ”¹ Reranking SonuÃ§larÄ±:")
    for i, (doc, score) in enumerate(ranked_results):
        print(f"      {i+1}. Skor: {score:.4f} | Metin: {doc.page_content[:50]}...")
        
    # En iyi sonucu dÃ¶ndÃ¼r
    return ranked_results[0][0]

# --- 4. TEST ZAMANI ---

# Test 1: Anlamsal Soru (VektÃ¶r bunu sever)
best_doc = search_system("Ä°nsan zihnini bilgisayara aktarma projesi nerede?")

# Test 2: Kod AdÄ± Sorusu (BM25 bunu sever, VektÃ¶r bazen kaÃ§Ä±rÄ±r)
best_doc_2 = search_system("X-99 ÅŸifresi nedir?")

# --- 5. GENERATION (Cevap Ãœretme) ---
print("\nğŸ¤– LLM Cevap Ãœretiyor...")
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

final_prompt = f"""
BaÄŸlam bilgisini kullanarak soruyu cevapla:
BaÄŸlam: {best_doc_2.page_content}
Soru: X-99 ÅŸifresi nedir?
"""
response = llm.invoke(final_prompt)
print(f"ğŸ SONUÃ‡: {response.content}")